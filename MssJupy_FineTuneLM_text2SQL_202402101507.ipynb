{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951a39d-125f-4d81-b536-9c8edd2d4108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bebd44c-643e-4f43-a6d8-3f20a87c78c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#\n",
    "# Fine-tune LLM using: huggingface TRL SFTTrainer, PEFT, BNB\n",
    "# LLM: \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# Task: generate executable SQL for the given 'plain English' request\n",
    "# MSS 20240210\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48c7ec-2bb4-40b2-ab05-39e3beda3f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "600d14b3-5487-4816-be0e-eb152a827bc6",
   "metadata": {},
   "source": [
    "# table of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969ff51-e32f-4178-bb41-eae9249f7902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad731ff-e8c1-4cc1-8c59-38665430d5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e6da70b-66c1-459e-918b-0e0113dad726",
   "metadata": {},
   "source": [
    "# read prepared trainset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa43832c-d5db-4689-bd6e-78b05fbb11d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ghtw30s ghtw30s 1182876 Feb 10 09:02 test_dataset.json\n",
      "-rw-rw-r-- 1 ghtw30s ghtw30s 4755289 Feb 10 09:02 train_dataset.json\n"
     ]
    }
   ],
   "source": [
    "!ls -ltA *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a3053d-c08d-4e2a-a096-e9f22a2bfc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 11 14:09:50 2024\n",
      "Sun Feb 11 14:09:51 2024\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "# Load jsonl data from disk\n",
    "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86fe11-df2e-42cc-a884-c4898c293baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a32a85cf-8b63-4621-8cbc-ed174b56ce89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# mss: inspect data format sent to llm for training\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0efc156e-8d22-432e-ac23-13410e0755c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_90 (team_1 VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Name the 2nd leg for team 1 of hamburg', 'role': 'user'},\n",
       "   {'content': 'SELECT 2 AS nd_leg FROM table_name_90 WHERE team_1 = \"hamburg\"',\n",
       "    'role': 'assistant'}],\n",
       "  [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_54 (season VARCHAR, lead VARCHAR, third VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'what is the season when the lead is john shuster and third is shawn rojeski?',\n",
       "    'role': 'user'},\n",
       "   {'content': 'SELECT season FROM table_name_54 WHERE lead = \"john shuster\" AND third = \"shawn rojeski\"',\n",
       "    'role': 'assistant'}],\n",
       "  [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_75 (category VARCHAR, director VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Tell me the category of na director', 'role': 'user'},\n",
       "   {'content': 'SELECT category FROM table_name_75 WHERE director = \"na\"',\n",
       "    'role': 'assistant'}],\n",
       "  [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_25 (directed___undirected VARCHAR, induced___non_induced VARCHAR, name VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'What is the directed/undirected of fpf (mavisto), which has an induced/non-induced of induced?',\n",
       "    'role': 'user'},\n",
       "   {'content': 'SELECT directed___undirected FROM table_name_25 WHERE induced___non_induced = \"induced\" AND name = \"fpf (mavisto)\"',\n",
       "    'role': 'assistant'}],\n",
       "  [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_10 (poll VARCHAR, wk_13 VARCHAR, wk_10 VARCHAR, wk_2 VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Which poll had a week 10 larger than 2, a week 2 of exactly 12, and a week 13 of 8?',\n",
       "    'role': 'user'},\n",
       "   {'content': 'SELECT poll FROM table_name_10 WHERE wk_10 > 2 AND wk_2 = \"12\" AND wk_13 = 8',\n",
       "    'role': 'assistant'}]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[34:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07188ccf-ab93-4a77-869e-6fcafb76be4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24752c91-5daa-4236-8836-91fe4cd8fb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d3b24c-aebf-4a72-84bb-0806d3f2a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 14:10:25.014218: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-11 14:10:25.062786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-11 14:10:25.062833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-11 14:10:25.064069: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-11 14:10:25.071893: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-11 14:10:26.309996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "Sun Feb 11 14:10:27 2024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c00c5ebb4f40618f83304de632876e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 11 14:11:17 2024\n",
      "Sun Feb 11 14:11:17 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hugging Face model id\n",
    "#https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# TRL - Transformer Reinforcement Learning   https://huggingface.co/docs/trl/en/index\n",
    "from trl import setup_chat_format\n",
    "\n",
    "# QA\n",
    "print('model_id' ,model_id)\n",
    "\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  *** REMOVE IF YOU START FROM A FINE-TUNED MODEL *** !!!\n",
    "\n",
    "# set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "\"\"\"\n",
    "https://huggingface.co/docs/trl/en/sft_trainer\n",
    "The setup_chat_format() function in trl easily sets up a model and tokenizer for conversational AI tasks. This function:\n",
    "    Adds special tokens to the tokenizer, e.g. <|im_start|> and <|im_end|>, to indicate the start and end of a conversation.\n",
    "    Resizes the model’s embedding layer to accommodate the new tokens.\n",
    "    Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI.\n",
    "    optionally you can pass resize_to_multiple_of to resize the embedding layer to a multiple of the resize_to_multiple_of argument, e.g. 64. If you want to see more formats being supported in the future, please open a GitHub issue on trl\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0faf6aa-d88a-4eb5-b880-ac00e8ffd396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e435440-d003-4dbc-bf84-450a09548891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c588ed2-a3dd-4f48-864b-ac3f3339a792",
   "metadata": {},
   "source": [
    "# SFTTrainer integration with peft, efficiently tune LLMs using QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeadd37d-0cf4-4682-97b0-972afb980efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 11 14:11:50 2024\n",
      "Sun Feb 11 14:11:50 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "    \n",
    "    lora_alpha=128,\n",
    "\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    r=256,\n",
    "    \n",
    "    bias=\"none\",\n",
    "    \n",
    "    target_modules=\"all-linear\",\n",
    "        \n",
    "    task_type=\"CAUSAL_LM\", \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b76a72c4-c791-4eed-b3ca-bdd4a08821d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=256, target_modules='all-linear', lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197e320-e8f7-4322-8205-944b7cde744a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfbe808-c504-4919-9f4b-0b4d94d3a955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1ee2300-9e2f-41db-9edd-50a79ac77fa9",
   "metadata": {},
   "source": [
    "# define the hyperparameters (TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ab1839a-6a45-4217-b482-726a597d531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dir to save checkpoints:  /media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/\n",
      "Sun Feb 11 14:12:57 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# better to make 'output_dir' uniq (so prior sessions files are preserved), especially when >1 code running in parallel\n",
    "\n",
    "## auto name\n",
    "# parm\n",
    "FlPthBase = \"/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/\"\n",
    "#\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "#\n",
    "epoch_time = int(time.time())\n",
    "current_date = datetime.datetime.now()\n",
    "yymmdd = int(current_date.strftime(\"%Y%m%d%H%M%S\"))\n",
    "rndint = random.randint(100, 999)\n",
    "\n",
    "FlPth = FlPthBase + 'MergedLora_' + str(yymmdd) + '_' + str(epoch_time) + '_' + str(rndint) + '/'\n",
    "print('' , 'dir to save checkpoints: ' , FlPth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "# MssJupy_FineTune_example_202402101507_2.ipynb : push_to_hub=Fale; 2.use more of gpu ram!\n",
    "# MSS 20240211: to prevent pusshing to hugfac hub, ==prevent needing internet, then set push_to_hub to False:\n",
    "args = TrainingArguments(\n",
    "\n",
    "    output_dir= FlPth , # directory to save and repository id\n",
    "\n",
    "    push_to_hub=False,                      # MSS 20240211: to prevent pusshing to hugfac hub, ==prevent needing internet, then set push_to_hub to False:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=3,\n",
    "\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    \n",
    "    #optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    optim=\"paged_adamw_8bit\",              # use fused adamw optimizer\n",
    "    \n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    #bf16=True,                              # use bfloat16 precision\n",
    "    #tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc74237-db08-48d8-9233-7a05d7b8a956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=2,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=no,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=2,\n",
       "gradient_checkpointing=True,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=0.0002,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/runs/Feb11_14-12-57_ghtw30s-WS-E900-G4-WS980T,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=10,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=constant,\n",
       "max_grad_norm=0.3,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=paged_adamw_8bit,\n",
       "optim_args=None,\n",
       "output_dir=/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=3,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=False,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.03,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78150633-5617-4131-baba-e8674fc6f7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6f111-c8bb-4dda-9cee-28bdf8020568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a50cb-ef9d-404d-88d7-f588d2b18b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daadd972-2d89-446a-ba09-f9cdd9f1f003",
   "metadata": {},
   "source": [
    "# create our SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "051b3ffc-18c0-4d0c-909b-ca10c103bf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 11 14:13:47 2024\n",
      "Sun Feb 11 14:14:30 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "max_seq_length = 3072\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19368e-877e-494c-aa24-6560e18b527b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce9a10-0cf1-4700-b518-5f8c4c4d9233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b9f64-6bbe-45f1-b663-e96ef715f85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b990e85-5b38-4b0b-b357-e50875f8fec6",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f052ad8-bfea-41ce-9e52-a5f76c9d85b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Sun Feb 11 14:16:27 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ghtw30s/virenv20231122/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 2:56:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.456400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.448800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghtw30s/virenv20231122/lib/python3.10/site-packages/peft/utils/save_and_load.py:160: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 Sun Feb 11 17:15:51 2024\n",
      "Took 10764.772048711777 seconds to run end-to-end.\n",
      "99 Sun Feb 11 17:15:51 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# e2e time\n",
    "start00 = time.time()\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "print('22' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "# e2e time\n",
    "end00 = time.time()\n",
    "#print(\"Took {} seconds to pull {} websites.\".format(end11 - start11, len(work101)))\n",
    "print(\"Took {} seconds to run end-to-end.\".format(end00 - start00 ))\n",
    "#\n",
    "TimeTookE2E = end00 - start00\n",
    "\n",
    "\n",
    "print( '99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb971d-5a7d-4f8b-ac3c-9395999f2881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e6288e5-c26d-4461-97e4-42ece47fd07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           1.2Ti        28Gi       904Gi        94Mi       326Gi       1.2Ti\n",
      "Swap:          2.0Gi          0B       2.0Gi\n",
      "Total:         1.2Ti        28Gi       906Gi\n",
      "Sun Feb 11 18:24:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000                Off | 00000000:3B:00.0  On |                  Off |\n",
      "| 34%   33C    P8              33W / 260W |  48556MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000                Off | 00000000:5E:00.0 Off |                  Off |\n",
      "| 33%   25C    P8              11W / 260W |  48588MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                          592MiB |\n",
      "|    0   N/A  N/A      3912      G   /usr/bin/gnome-shell                        113MiB |\n",
      "|    0   N/A  N/A      6232      G   ...irefox/3779/usr/lib/firefox/firefox      202MiB |\n",
      "|    0   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    44798MiB |\n",
      "|    1   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    45918MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1065.0, 6.0, 'Sun Feb 11 14:06:05 2024'],\n",
       " [12078.0, 13946.0, 'Sun Feb 11 14:11:18 2024'],\n",
       " [48556.0, 48588.0, 'Sun Feb 11 18:24:11 2024']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record ram usage cpu gpu\n",
    "!free -g -h -t\n",
    "!nvidia-smi\n",
    "ArrGpuRamUsage.append( GetGpuRamUsage() )\n",
    "ArrGpuRamUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95103ac3-b86a-4931-9a5b-6668a40c5c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffd228cc-bf08-4a0e-aca6-2a9479d3b220",
   "metadata": {},
   "source": [
    "# save tuned neural layers to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f7ab9ed-3fc5-4246-831f-b88baf125f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Sun Feb 11 18:24:18 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghtw30s/virenv20231122/lib/python3.10/site-packages/peft/utils/save_and_load.py:160: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 Sun Feb 11 18:26:43 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n",
    "\n",
    "\n",
    "\n",
    "print('99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adcc747d-a50c-4669-98cd-433e36157a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4562b66-3fd9-4b79-a5f6-1cac120fcc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b2967ae-c8d5-495f-a614-698a4d08ba53",
   "metadata": {},
   "source": [
    "# free the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d6306-b756-4490-9b42-5b72e2f8c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# since next section uses same names for model and etc, then it overwrites anyway!\n",
    "\n",
    "\n",
    "# free the memory again\n",
    "\"\"\"\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfa365-57dc-4aa7-ae2d-67c678bd9be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7359c21a-ab2b-46b3-8ff9-43c16afaf0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/:\n",
      "total 16165678\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        4920 Feb 11 18:26 training_args.bin\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s     1795677 Feb 11 18:26 tokenizer.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s      493443 Feb 11 18:26 tokenizer.model\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s          51 Feb 11 18:26 added_tokens.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s         557 Feb 11 18:26 special_tokens_map.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        1606 Feb 11 18:26 tokenizer_config.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s         684 Feb 11 18:26 adapter_config.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 16551333320 Feb 11 18:26 adapter_model.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        5112 Feb 11 18:24 README.md\n",
      "drwxrwxrwx 1 ghtw30s ghtw30s         408 Feb 11 17:15 checkpoint-67\n",
      "drwxrwxrwx 1 ghtw30s ghtw30s         216 Feb 11 14:16 runs\n",
      "\n",
      "/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/checkpoint-67:\n",
      "total 23752614\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        1106 Feb 11 17:15 trainer_state.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s       14244 Feb 11 17:15 rng_state.pth\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        1064 Feb 11 17:15 scheduler.pt\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s  7768996954 Feb 11 17:15 optimizer.pt\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        4920 Feb 11 17:14 training_args.bin\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s     1795677 Feb 11 17:14 tokenizer.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s      493443 Feb 11 17:14 tokenizer.model\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s          51 Feb 11 17:14 added_tokens.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s         557 Feb 11 17:14 special_tokens_map.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        1606 Feb 11 17:14 tokenizer_config.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s         684 Feb 11 17:14 adapter_config.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 16551333320 Feb 11 17:14 adapter_model.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        5112 Feb 11 17:11 README.md\n",
      "\n",
      "/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/runs:\n",
      "total 0\n",
      "drwxrwxrwx 1 ghtw30s ghtw30s 264 Feb 11 14:16 Feb11_14-12-57_ghtw30s-WS-E900-G4-WS980T\n",
      "\n",
      "/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/runs/Feb11_14-12-57_ghtw30s-WS-E900-G4-WS980T:\n",
      "total 8\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 6469 Feb 11 17:15 events.out.tfevents.1707678987.ghtw30s-WS-E900-G4-WS980T.71170.0\n",
      "Sun Feb 11 07:27:24 PM EST 2024\n"
     ]
    }
   ],
   "source": [
    "!ls -ltAR $args.output_dir ; date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b08ae-cf73-47d0-bc00-b30db4c64877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418a64e-b0b1-4ce6-b3bf-7ee60bab946c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10f76635-a2cf-4e81-b66f-810633d3ca6e",
   "metadata": {},
   "source": [
    "# Merge LoRA adapter in to the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f1608-9e06-4316-a61c-bdb6fe419432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "When using QLoRA, we only train adapters and not the full model. \n",
    "This means when saving the model during training we only save the adapter weights and not the full model. \n",
    "If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights \n",
    "into the model weights using the merge_and_unload method and then save the model with the save_pretrained method. \n",
    "This will save a default model, which can be used for inference.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cbdd761-7d13-4938-9eb5-c5c6a541ece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Sun Feb 11 19:27:59 2024\n",
      "11 Sun Feb 11 19:27:59 2024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4cc134de454ac887efd90b385d04fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Sun Feb 11 19:28:26 2024\n",
      "99 Sun Feb 11 19:28:26 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "config = PeftConfig.from_pretrained(args.output_dir)\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,low_cpu_mem_usage=True)\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "\n",
    "\n",
    "print('99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb98289a-6883-4428-91ea-f9ca58ec7562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           1.2Ti       203Gi       697Gi        96Mi       357Gi       1.0Ti\n",
      "Swap:          2.0Gi          0B       2.0Gi\n",
      "Total:         1.2Ti       203Gi       699Gi\n",
      "Sun Feb 11 19:28:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000                Off | 00000000:3B:00.0  On |                  Off |\n",
      "| 34%   33C    P8              35W / 260W |  21630MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000                Off | 00000000:5E:00.0 Off |                  Off |\n",
      "| 33%   25C    P8              12W / 260W |  25162MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                          592MiB |\n",
      "|    0   N/A  N/A      3912      G   /usr/bin/gnome-shell                        123MiB |\n",
      "|    0   N/A  N/A      6232      G   ...irefox/3779/usr/lib/firefox/firefox      197MiB |\n",
      "|    0   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    17892MiB |\n",
      "|    1   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    22492MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1065.0, 6.0, 'Sun Feb 11 14:06:05 2024'],\n",
       " [12078.0, 13946.0, 'Sun Feb 11 14:11:18 2024'],\n",
       " [48556.0, 48588.0, 'Sun Feb 11 18:24:11 2024'],\n",
       " [21630.0, 25162.0, 'Sun Feb 11 19:28:35 2024']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record ram usage cpu gpu\n",
    "!free -g -h -t\n",
    "!nvidia-smi\n",
    "ArrGpuRamUsage.append( GetGpuRamUsage() )\n",
    "ArrGpuRamUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e96cd592-aec9-456d-9afb-5780334154b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59038ea-da36-4566-895b-2ac0dbc1c372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51478e03-5c60-4557-8576-19dcd350dfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Sun Feb 11 19:30:12 2024\n",
      "13 Sun Feb 11 19:30:16 2024\n",
      "15 Sun Feb 11 19:31:09 2024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897227eabdce49c49bca81aa15b538aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 Sun Feb 11 19:33:14 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "#\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print('13' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "print('15' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print('99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "141f8c94-e1b9-4e44-86a9-6683bb0e2783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           1.2Ti       136Gi       764Gi        84Mi       357Gi       1.1Ti\n",
      "Swap:          2.0Gi          0B       2.0Gi\n",
      "Total:         1.2Ti       136Gi       766Gi\n",
      "Sun Feb 11 19:33:20 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000                Off | 00000000:3B:00.0  On |                  Off |\n",
      "| 34%   37C    P2              70W / 260W |  37316MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000                Off | 00000000:5E:00.0 Off |                  Off |\n",
      "| 33%   25C    P8              11W / 260W |  25162MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                          592MiB |\n",
      "|    0   N/A  N/A      3912      G   /usr/bin/gnome-shell                        123MiB |\n",
      "|    0   N/A  N/A      6232      G   ...irefox/3779/usr/lib/firefox/firefox      208MiB |\n",
      "|    0   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    33566MiB |\n",
      "|    1   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    22492MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1065.0, 6.0, 'Sun Feb 11 14:06:05 2024'],\n",
       " [12078.0, 13946.0, 'Sun Feb 11 14:11:18 2024'],\n",
       " [48556.0, 48588.0, 'Sun Feb 11 18:24:11 2024'],\n",
       " [21630.0, 25162.0, 'Sun Feb 11 19:28:35 2024'],\n",
       " [37316.0, 25162.0, 'Sun Feb 11 19:33:21 2024']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record ram usage cpu gpu\n",
    "!free -g -h -t\n",
    "!nvidia-smi\n",
    "ArrGpuRamUsage.append( GetGpuRamUsage() )\n",
    "ArrGpuRamUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd97d1-546a-434d-913c-ecb6c6aba61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d1d03-cd82-44dd-b9fa-aeece67296d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f1ed783-f71a-451f-a741-6db810ae4a19",
   "metadata": {},
   "source": [
    "# save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfc31695-f62d-48e9-943f-d3317565983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.output_dir /media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211141257_1707678777_508/\n",
      " dir to save model:  /media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211193450_1707698090_754\n",
      "args.output_dir /media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211193450_1707698090_754\n",
      "99 Sun Feb 11 19:34:50 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# uniq dir to save\n",
    "\n",
    "print( 'args.output_dir' ,args.output_dir)\n",
    "\n",
    "## auto name\n",
    "# parm\n",
    "FlPthBase = \"/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/\"\n",
    "#\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "#\n",
    "epoch_time = int(time.time())\n",
    "current_date = datetime.datetime.now()\n",
    "yymmdd = int(current_date.strftime(\"%Y%m%d%H%M%S\"))\n",
    "rndint = random.randint(100, 999)\n",
    "\n",
    "FlPth = FlPthBase + 'MergedLora_' + str(yymmdd) + '_' + str(epoch_time) + '_' + str(rndint) + ''\n",
    "print('' , 'dir to save model: ' , FlPth)\n",
    "#\n",
    "args.output_dir = FlPth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print( 'args.output_dir' ,args.output_dir)\n",
    "#args.output_dir /media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240210093118_1707575478_896\n",
    "\n",
    "\n",
    "print('99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e7129-ea3f-4363-b02d-b279e70f5e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffbed76b-fb7a-48cc-9777-83698394f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Sun Feb 11 19:35:03 2024\n",
      "13 Sun Feb 11 19:36:05 2024\n",
      "99 Sun Feb 11 19:48:00 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "print('13' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36ea3283-e0c7-42cd-9944-5962bb56d192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           1.2Ti       137Gi       587Gi        84Mi       534Gi       1.1Ti\n",
      "Swap:          2.0Gi          0B       2.0Gi\n",
      "Total:         1.2Ti       137Gi       589Gi\n",
      "Sun Feb 11 19:48:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000                Off | 00000000:3B:00.0  On |                  Off |\n",
      "| 34%   33C    P8              34W / 260W |  37290MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000                Off | 00000000:5E:00.0 Off |                  Off |\n",
      "| 33%   25C    P8              11W / 260W |  25162MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                          592MiB |\n",
      "|    0   N/A  N/A      3912      G   /usr/bin/gnome-shell                        113MiB |\n",
      "|    0   N/A  N/A      6232      G   ...irefox/3779/usr/lib/firefox/firefox      193MiB |\n",
      "|    0   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    33566MiB |\n",
      "|    1   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    22492MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1065.0, 6.0, 'Sun Feb 11 14:06:05 2024'],\n",
       " [12078.0, 13946.0, 'Sun Feb 11 14:11:18 2024'],\n",
       " [48556.0, 48588.0, 'Sun Feb 11 18:24:11 2024'],\n",
       " [21630.0, 25162.0, 'Sun Feb 11 19:28:35 2024'],\n",
       " [37316.0, 25162.0, 'Sun Feb 11 19:33:21 2024'],\n",
       " [37290.0, 25162.0, 'Sun Feb 11 19:48:06 2024']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record ram usage cpu gpu\n",
    "!free -g -h -t\n",
    "!nvidia-smi\n",
    "ArrGpuRamUsage.append( GetGpuRamUsage() )\n",
    "ArrGpuRamUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e44657-4184-4833-9fa2-63562380f8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7ee65cb-8f70-45f9-bba6-f22a9457e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87G\t/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211193450_1707698090_754\n",
      "Sun Feb 11 07:48:09 PM EST 2024\n",
      "total 91216713\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s      92658 Feb 11 19:48 model.safetensors.index.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s  614507328 Feb 11 19:48 model-00048-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:47 model-00047-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:47 model-00046-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:47 model-00045-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:47 model-00044-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:46 model-00043-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:46 model-00042-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:46 model-00041-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:46 model-00040-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:46 model-00039-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:45 model-00038-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:45 model-00037-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:44 model-00036-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:44 model-00035-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:44 model-00034-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:44 model-00033-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963002512 Feb 11 19:43 model-00032-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996507568 Feb 11 19:43 model-00031-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:42 model-00030-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:42 model-00029-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:42 model-00028-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:42 model-00027-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:42 model-00026-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:41 model-00025-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:41 model-00024-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:41 model-00023-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:40 model-00022-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:40 model-00021-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:40 model-00020-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:40 model-00019-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:40 model-00018-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:39 model-00017-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:39 model-00016-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:39 model-00015-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019096 Feb 11 19:39 model-00014-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490936 Feb 11 19:38 model-00013-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:38 model-00012-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019096 Feb 11 19:38 model-00011-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490936 Feb 11 19:38 model-00010-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:37 model-00009-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019096 Feb 11 19:37 model-00008-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963002488 Feb 11 19:37 model-00007-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996507544 Feb 11 19:37 model-00006-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:37 model-00005-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019096 Feb 11 19:36 model-00004-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490936 Feb 11 19:36 model-00003-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:36 model-00002-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1990281696 Feb 11 19:36 model-00001-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        111 Feb 11 19:36 generation_config.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        773 Feb 11 19:36 config.json\n"
     ]
    }
   ],
   "source": [
    "!du -hs $args.output_dir ; date\n",
    "!ls -ltA $args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b26de4-fc93-4d00-8b50-a7ae528a1537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43171e69-cdbd-4805-b155-f6490ec85c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce268c87-964a-456c-b8e8-6f74352a1099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e0b63d-f00d-46c1-93e7-87aa1f2d3cd1",
   "metadata": {},
   "source": [
    "# Test Model and run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adbc018e-60d8-4aae-a05a-4273d26a6258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           1.2Ti       139Gi       583Gi       112Mi       536Gi       1.1Ti\n",
      "Swap:          2.0Gi          0B       2.0Gi\n",
      "Total:         1.2Ti       139Gi       585Gi\n",
      "Mon Feb 12 09:22:03 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000                Off | 00000000:3B:00.0  On |                  Off |\n",
      "| 34%   34C    P8              42W / 260W |  37336MiB / 49152MiB |     36%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000                Off | 00000000:5E:00.0 Off |                  Off |\n",
      "| 33%   25C    P8              11W / 260W |  25162MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                          612MiB |\n",
      "|    0   N/A  N/A      3912      G   /usr/bin/gnome-shell                        114MiB |\n",
      "|    0   N/A  N/A      6232      G   ...irefox/3779/usr/lib/firefox/firefox      218MiB |\n",
      "|    0   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    33566MiB |\n",
      "|    1   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    22492MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1065.0, 6.0, 'Sun Feb 11 14:06:05 2024'],\n",
       " [12078.0, 13946.0, 'Sun Feb 11 14:11:18 2024'],\n",
       " [48556.0, 48588.0, 'Sun Feb 11 18:24:11 2024'],\n",
       " [21630.0, 25162.0, 'Sun Feb 11 19:28:35 2024'],\n",
       " [37316.0, 25162.0, 'Sun Feb 11 19:33:21 2024'],\n",
       " [37290.0, 25162.0, 'Sun Feb 11 19:48:06 2024'],\n",
       " [37336.0, 25162.0, 'Mon Feb 12 09:22:04 2024']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record ram usage cpu gpu\n",
    "!free -g -h -t\n",
    "!nvidia-smi\n",
    "ArrGpuRamUsage.append( GetGpuRamUsage() )\n",
    "ArrGpuRamUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18d17a1b-de58-4c6c-a9b4-3c0946ee47cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/ghtw30s/SSD-PUT/mss20230718/LLM/EXECUTE_ExecuteOnAsusG4/FineTune/models/MergedLora_20240211193450_1707698090_754'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca7e9f85-7f8b-4661-b048-7ed7473f8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 91216713\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s      92658 Feb 11 19:48 model.safetensors.index.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s  614507328 Feb 11 19:48 model-00048-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:47 model-00047-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:47 model-00046-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:47 model-00045-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:47 model-00044-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:46 model-00043-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:46 model-00042-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:46 model-00041-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:46 model-00040-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:46 model-00039-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:45 model-00038-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:45 model-00037-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:44 model-00036-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:44 model-00035-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:44 model-00034-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:44 model-00033-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963002512 Feb 11 19:43 model-00032-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996507568 Feb 11 19:43 model-00031-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:42 model-00030-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:42 model-00029-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:42 model-00028-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:42 model-00027-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:42 model-00026-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:41 model-00025-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:41 model-00024-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:41 model-00023-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:40 model-00022-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:40 model-00021-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:40 model-00020-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:40 model-00019-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019128 Feb 11 19:40 model-00018-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019120 Feb 11 19:39 model-00017-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490952 Feb 11 19:39 model-00016-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:39 model-00015-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019096 Feb 11 19:39 model-00014-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490936 Feb 11 19:38 model-00013-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:38 model-00012-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019096 Feb 11 19:38 model-00011-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490936 Feb 11 19:38 model-00010-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:37 model-00009-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019096 Feb 11 19:37 model-00008-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963002488 Feb 11 19:37 model-00007-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996507544 Feb 11 19:37 model-00006-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:37 model-00005-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019096 Feb 11 19:36 model-00004-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1996490936 Feb 11 19:36 model-00003-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1963019104 Feb 11 19:36 model-00002-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s 1990281696 Feb 11 19:36 model-00001-of-00048.safetensors\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        111 Feb 11 19:36 generation_config.json\n",
      "-rwxrwxrwx 1 ghtw30s ghtw30s        773 Feb 11 19:36 config.json\n",
      "Mon Feb 12 09:23:21 AM EST 2024\n"
     ]
    }
   ],
   "source": [
    "!ls -ltA $args.output_dir ; date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b5b46-7573-4071-8e03-88ba827c12bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656b9f6-798a-4c64-a811-f3178e2b572f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b03aa23a-56a1-4727-9474-67811137b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Mon Feb 12 10:01:44 2024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690048bfe1b047ec81c1a04167d46d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Mon Feb 12 10:02:40 2024\n",
      "99 Mon Feb 12 10:02:40 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#peft_model_id = \"./code-llama-7b-text-to-sql\"\n",
    "model_id = args.output_dir\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline \n",
    "\n",
    "\n",
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this to load llm that lora adapter is already merged with main nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "#\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    \n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    #RuntimeError: cutlassF: no kernel found to launch!\n",
    "    #https://github.com/Lightning-AI/lit-gpt/issues/327\n",
    "    \n",
    "    #quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('33' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b891429-772b-4750-9c9d-8665b06287b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bf305df-8dcb-42a4-b027-1f0f06216915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           1.2Ti       360Gi       362Gi       118Mi       536Gi       891Gi\n",
      "Swap:          2.0Gi          0B       2.0Gi\n",
      "Total:         1.2Ti       360Gi       364Gi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 12 10:03:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000                Off | 00000000:3B:00.0  On |                  Off |\n",
      "| 34%   37C    P8              35W / 260W |  40198MiB / 49152MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000                Off | 00000000:5E:00.0 Off |                  Off |\n",
      "| 33%   25C    P8              11W / 260W |  47346MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                          612MiB |\n",
      "|    0   N/A  N/A      3912      G   /usr/bin/gnome-shell                        123MiB |\n",
      "|    0   N/A  N/A      6232      G   ...irefox/3779/usr/lib/firefox/firefox      198MiB |\n",
      "|    0   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    36438MiB |\n",
      "|    1   N/A  N/A      3656      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     71170      C   ...e/ghtw30s/virenv20231122/bin/python    44676MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1065.0, 6.0, 'Sun Feb 11 14:06:05 2024'],\n",
       " [12078.0, 13946.0, 'Sun Feb 11 14:11:18 2024'],\n",
       " [48556.0, 48588.0, 'Sun Feb 11 18:24:11 2024'],\n",
       " [21630.0, 25162.0, 'Sun Feb 11 19:28:35 2024'],\n",
       " [37316.0, 25162.0, 'Sun Feb 11 19:33:21 2024'],\n",
       " [37290.0, 25162.0, 'Sun Feb 11 19:48:06 2024'],\n",
       " [37336.0, 25162.0, 'Mon Feb 12 09:22:04 2024'],\n",
       " [27596.0, 47346.0, 'Mon Feb 12 09:38:07 2024'],\n",
       " [40198.0, 47346.0, 'Mon Feb 12 10:03:54 2024']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record ram usage cpu gpu\n",
    "!free -g -h -t\n",
    "!nvidia-smi\n",
    "ArrGpuRamUsage.append( GetGpuRamUsage() )\n",
    "ArrGpuRamUsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b228c8-10d9-40d7-8471-127ea9ab3e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfab6b8-3b9a-4154-895e-c6d4b2ad3452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605f780-2876-4c30-a32c-3f313ec7f6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1415e665-9879-4374-ac17-4895d1444d5b",
   "metadata": {},
   "source": [
    "# Let’s load our test dataset try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95a86a1f-c847-4079-a28d-0eff9c0edcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Mon Feb 12 09:47:57 2024\n",
      "11 Mon Feb 12 09:47:57 2024\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d2422-9472-49c0-9a55-5f6d4e63afa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2117f440-bc56-4bf9-9064-cf5dde81854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Mon Feb 12 10:03:59 2024\n",
      "11 Mon Feb 12 10:10:56 2024\n",
      "Query:\n",
      "How many patients do each physician take care of? List their names and number of patients they take care of.\n",
      "Original Answer:\n",
      "SELECT T1.name, COUNT(*) FROM physician AS T1 JOIN patient AS T2 ON T1.employeeid = T2.PCP GROUP BY T1.employeeid\n",
      "Generated Answer:\n",
      "SELECT T2.name, COUNT(*) FROM patient AS T1 JOIN physician AS T2 ON T1.PCP = T2.employeeid GROUP BY T2.name\n",
      "11 Mon Feb 12 10:10:56 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "# Test on sample \n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "#RuntimeError: cutlassF: no kernel found to launch!\n",
    "\n",
    "\n",
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
    "\n",
    "print('99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d47bbe-a889-4eb5-a839-8e4594917028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5be01347-c1b0-4c76-a2c6-e7644239008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE patient (PCP VARCHAR); CREATE TABLE physician (name VARCHAR, employeeid VARCHAR)',\n",
       "   'role': 'system'},\n",
       "  {'content': 'How many patients do each physician take care of? List their names and number of patients they take care of.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'SELECT T1.name, COUNT(*) FROM physician AS T1 JOIN patient AS T2 ON T1.employeeid = T2.PCP GROUP BY T1.employeeid',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6876e48-e73e-4daf-a395-8e8caf8e3d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE patient (PCP VARCHAR); CREATE TABLE physician (name VARCHAR, employeeid VARCHAR)',\n",
       "  'role': 'system'},\n",
       " {'content': 'How many patients do each physician take care of? List their names and number of patients they take care of.',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[rand_idx][\"messages\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a96291c-4ad9-4632-9d6c-7184135b0ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE patient (PCP VARCHAR); CREATE TABLE physician (name VARCHAR, employeeid VARCHAR)<|im_end|>\\n<|im_start|>user\\nHow many patients do each physician take care of? List their names and number of patients they take care of.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e256b-2344-41d5-ba85-5393e34cc226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7efdb0-7ac3-442e-93da-c7099c385c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a6f47-71c7-43fe-96df-b158eadad0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f70417f-379a-45a5-8562-bfdbc2544883",
   "metadata": {},
   "source": [
    "# eval(FTed LLM) over a set of testset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65511adf-34cc-4ab6-b526-f08b2a38c16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Mon Feb 12 10:32:06 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████████▎                                                                                                                  | 9/100 [59:35<10:04:03, 398.28s/it]/home/ghtw30s/virenv20231122/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [11:49:57<00:00, 425.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.00%\n",
      "99 Mon Feb 12 22:22:04 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(sample):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "success_rate = []\n",
    "\n",
    "\n",
    "number_of_eval_samples = 1000\n",
    "number_of_eval_samples = 100\n",
    "\n",
    "\n",
    "# iterate over eval dataset and predict\n",
    "for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n",
    "    success_rate.append(evaluate(s))\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91434ea-8d96-4f4e-85a0-7e73e81f19af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc6282-f17f-4112-bad9-8cd02162decf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef399b3f-1214-409c-9529-64a26a9161eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 19 08:29:34 2024\n",
      "Fri Jan 19 08:29:35 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, gc, re, warnings\n",
    "# Garbage Collection\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd9510-5d21-4faf-a543-4e8b7bf26006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virenv20231122j",
   "language": "python",
   "name": "virenv20231122j"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
